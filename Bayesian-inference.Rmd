---
title: "SOST 2024 Research Away Day"
author: "Arkadiusz Wi&#347;niowski"
date: "16 May 2024"
output:
  html_document:
    toc: yes
    toc_depth: '1'
    df_print: paged
    number_sections: yes
  pdf_document:
    toc: yes
    toc_depth: '1'
    number_sections: yes
  '#pdf_document': default
subtitle: 'Bayesian Inference'
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

>**Learning Objectives**<br>
>In this lab, you will learn about:<br>
>- Inference about the binomial model parameter.<br>
>- Setting up a prior distribution. <br>
>- Updating prior with data.<br>



# Bayesian inference in practice 

To illustrate how Bayesian inference works in practice, we are going to implement a binomial model which aims at estimating the probability of a baby being born in a given hospital. We want to know what the probability of the baby being a girl is. We can assume (and it's a reasonable assumption) that the outcome variable $Y$ which takes values 0 (boy) or 1 (girl) is generated by Bernoulli trials with some unknown parameter $\theta$ - the probability of the baby being a girl. This is well-described by a **binomial model**:  $P(girls=k, n|\theta)$.

  - It is nice because if we assume the prior to be Beta distribution, we can derive the posterior by hand.
  - By the way, what's the frequentist estimator of $\theta$?
  - Prior: $Beta(a,b)$ has expectation $a/(a+b)$
  - **posterior** is also Beta!
  - Posterior: $Beta(a+k, b+n-k)$, has expectation $(a+k)/(a+b+n)$
  - Note: when posterior looks like prior we call the prior **conjugate**

## Defining a prior distribution

What do (can) we know about the probability of the baby being a girl? Let's try some distributions. We know that the conjugate prior distribution is beta distribution.

```{r echo=F, warning = F, message = F}
library("tidyr")
library("dplyr")
library("stringr")
library("ggplot2")
priors = expand_grid(shape1 = c(0.1, 0.8, 1:5), shape2 = c(0.1, 0.8, 1:5)) %>%
          expand(nesting(shape1, shape2),
                 x = seq(from = 0, to = 1, length.out = 1000)) %>% 
          mutate(a     = str_c("a = ", shape1),
                 b     = str_c("b = ", shape2),
                 group = rep(1:1000, each = 49))

priors %>% ggplot(aes(x = x, group = group)) +
    geom_line(aes(y = dbeta(x, shape1 = shape1, shape2 = shape2)),
            color = "grey50", size = 1.25) +
    scale_x_continuous(expression(theta), breaks = c(0, .5, 1)) +
    coord_cartesian(ylim = c(0, 3)) +
    labs(title = "Beta distributions for selcting a prior",
         y = expression(P(theta*"|"*a*", "*b))) +
    theme(panel.grid = element_blank()) +
    facet_grid(b~a) + theme_bw()
```

We could define an informative prior based on demographic knowledge with expectation 0.4854369.

```{r echo=F, warning = F, message = F}
inf_prior = data.frame( th = seq(0, 1, by = 0.001), 
                        prob = dbeta(seq(0, 1, by = 0.001), 1000,1000*(1-0.4854369)/0.4854369))

ggplot(inf_prior, aes(x = th, group = 1)) +
    geom_line(aes(y = prob)) +
    scale_x_continuous(expression(theta), breaks = c(0, .5, 1)) +
    labs(title = "Informative Beta distribution prior",
         y = expression(P(theta*"|"*a*", "*b))) +
    theme(panel.grid = element_blank()) + theme_bw()
```

## Simulating data
Based on  demographic knowledge, we simulate data on the first thousand of babies born in some hospital since the beginning of year 2017 (1 denotes a girl, 0 - a boy).

```{r echo=T, warning = F, message = F}
set.seed(202)
girl = rbinom(n = 1000, size = 1, prob = 0.4854369)

```

a. Compute and plot the mean for the first 20 observations, then 50, then 100...
b. Plot the data to see the distribution of the simulated data of different sample sizes.
c. What is the frequentist estimator of Î¸ for each sample size?

## Plotting posterior distributions

a. What is the posterior distribution of parameter $\theta$ if we use conjugate priors?
b. What are the posteriors given the prior distributions that we have proposed earlier?
c. Plot them along the priors and data, using the following code.
```{r echo=T, warning = F, message = F}
a=5; b=5; th = seq(0, 1, by = 0.001)
k = sum(girl[1:20]) # sample size:  20 observations
n = length(girl[1:20])

plot(th, dbeta(th, a, b), ylim = c(0,5), col = "darkgrey" , t = "l")   #Beta(5,5) - prior
# plotting likelihood for 20 observations
lines(th, dbeta(th,k+1,n-k+1), col="red", t="l") 
# plotting posterior distribution
lines(th,dbeta(th, a+k, b+n-k), col="black", lwd=3, t="l")
```

d. Now, we keep the same prior, but increase sample size to $n=100$
```{r echo = F, warning = F, message = F, eval = F}
# plotting prior
a=5; b=5; th = seq(0, 1, by = 0.001)
k = sum(girl[1:100]) # sample size:  100 observations
n = length(girl[1:100])

plot(th, dbeta(th, a, b), ylim = c(0,8), col = "darkgrey", t = "l")   #Beta(5, 5) - prior
# plotting likelihood for 100 observations
lines(th, dbeta(th, k+1, n-k+1), col = "red", t = "l") 
# plotting posterior distribution
lines(th, dbeta(th, a+k, b+n-k), col = "black", lwd = 3, t = "l")
```

e. Plot prior, likelihood and posterior for the informative prior, where $a=1000, b=1000(1-0.4854369)/0.4854369)$. Below is a little helper with the prior.

```{r warning = F, message = F, eval = F}
plot(th,dbeta(th,0.5,0.5),ylim=c(0,30),col="darkgrey",t="l")   #Beta(0.5,0.5)
plot(th,dbeta(th,1000,1000*(1-0.4854369)/0.4854369),ylim=c(0,400),col="darkgrey",t="l") #informative prior with expectation 0.4854369
```

  f. How does our inference change when we take sample size $n=50$, $n=100$, $n=1000$?
  e. Use prior $a=0.5, b=0.5$ 
