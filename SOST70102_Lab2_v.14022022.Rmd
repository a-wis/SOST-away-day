---
title: "SOST70102 Demographic Forecasting"
author: "Arkadiusz Wi&#347;niowski"
date: "17 February 2022"
output:
  html_document:
    toc: yes
    toc_depth: '1'
    df_print: paged
    number_sections: yes
  pdf_document:
    toc: yes
    toc_depth: '1'
    number_sections: yes
  '#pdf_document': default
subtitle: 'Lab 2: Bayesian inference in Demography'
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

>**Learning Objectives**<br>
>In this lab, you will learn :<br>
>- Bayesian inference in practice.<br>
>- The basics of `Stan` and `RStan`,<br>
>- Specifying a binomial model in `Stan` and `RStan`.<br>


# Bayesian inference in practice 

To illustrate how Bayesian inference works in practice, we are going to implement a binomial model which aims at estimating the probability of a baby being born in a given hospital. We want to know what the probability of the baby being a girl is. We can assume (and it's a reasonable assumption) that the outcome variable $Y$ which takes values 0 (boy) or 1 (girl) is generated by Bernoulli trials with some unknown parameter $\theta$ - the probability of the baby being a girl. This is well-described by a **binomial model**:  $P(girls=k, n|\theta)$.

  - It is nice because if we assume the prior to be Beta distribution, we can derive the posterior by hand.
  - By the way, what's the frequentist estimator of $\theta$?
  - Prior: $Beta(a,b)$ has expectation $a/(a+b)$
  - **posterior** is also Beta!
  - Posterior: $Beta(a+k, b+n-k)$, has expectation $(a+k)/(a+b+n)$
  - Note: when posterior looks like prior we call the prior **conjugate**

## Defining a prior distribution

What do (can) we know about the probability of the baby being a girl? Let's try some distributions. We know that the conjugate prior distribution is beta distribution.

```{r echo=F, warning = F, message = F}
library("tidyr")
library("dplyr")
library("stringr")
library("ggplot2")
priors = expand_grid(shape1 = c(0.1, 0.8, 1:5), shape2 = c(0.1, 0.8, 1:5)) %>%
          expand(nesting(shape1, shape2),
                 x = seq(from = 0, to = 1, length.out = 1000)) %>% 
          mutate(a     = str_c("a = ", shape1),
                 b     = str_c("b = ", shape2),
                 group = rep(1:1000, each = 49))

priors %>% ggplot(aes(x = x, group = group)) +
    geom_line(aes(y = dbeta(x, shape1 = shape1, shape2 = shape2)),
            color = "grey50", size = 1.25) +
    scale_x_continuous(expression(theta), breaks = c(0, .5, 1)) +
    coord_cartesian(ylim = c(0, 3)) +
    labs(title = "Beta distributions for selcting a prior",
         y = expression(P(theta*"|"*a*", "*b))) +
    theme(panel.grid = element_blank()) +
    facet_grid(b~a) + theme_bw()
```

We could define an informative prior based on demographic knowledge with expectation 0.4854369.

```{r echo=F, warning = F, message = F}
inf_prior = data.frame( th = seq(0, 1, by = 0.001), 
                        prob = dbeta(seq(0, 1, by = 0.001), 1000,1000*(1-0.4854369)/0.4854369))

ggplot(inf_prior, aes(x = th, group = 1)) +
    geom_line(aes(y = prob)) +
    scale_x_continuous(expression(theta), breaks = c(0, .5, 1)) +
    labs(title = "Informative Beta distribution prior",
         y = expression(P(theta*"|"*a*", "*b))) +
    theme(panel.grid = element_blank()) + theme_bw()
```

## Simulating data
Based on  demographic knowledge, we simulate data on the first thousand of babies born in some hospital since the beginning of year 2017 (1 denotes a girl, 0 - a boy).

```{r echo=T, warning = F, message = F}
set.seed(202)
girl = rbinom(n = 1000, size = 1, prob = 0.4854369)

```

a. Compute and plot the mean for the first 20 observations, then 50, then 100...
b. Plot the data to see the distribution of the simulated data of different sample sizes.
c. What is the frequentist estimator of Î¸ for each sample size?

## Plotting posterior distributions

a. What is the posterior distribution of parameter $\theta$ if we use conjugate priors?
b. What are the posteriors given the prior distributions that we have proposed earlier?
c. Plot them along the priors and data, using the following code.
```{r echo=T, warning = F, message = F}
a=5; b=5; th = seq(0, 1, by = 0.001)
k = sum(girl[1:20]) # sample size:  20 observations
n = length(girl[1:20])

plot(th, dbeta(th, a, b), ylim = c(0,5), col = "darkgrey" , t = "l")   #Beta(5,5) - prior
# plotting likelihood for 20 observations
lines(th, dbeta(th,k+1,n-k+1), col="red", t="l") 
# plotting posterior distribution
lines(th,dbeta(th, a+k, b+n-k), col="black", lwd=3, t="l")
```

d. Now, we keep the same prior, but increase sample size to $n=100$
```{r echo = F, warning = F, message = F, eval = F}
# plotting prior
a=5; b=5; th = seq(0, 1, by = 0.001)
k = sum(girl[1:100]) # sample size:  100 observations
n = length(girl[1:100])

plot(th, dbeta(th, a, b), ylim = c(0,8), col = "darkgrey", t = "l")   #Beta(5, 5) - prior
# plotting likelihood for 100 observations
lines(th, dbeta(th, k+1, n-k+1), col = "red", t = "l") 
# plotting posterior distribution
lines(th, dbeta(th, a+k, b+n-k), col = "black", lwd = 3, t = "l")
```

e. Plot prior, likelihood and posterior for the informative prior, where $a=1000, b=1000(1-0.4854369)/0.4854369)$. Below is a little helper with the prior.

```{r warning = F, message = F, eval = F}
plot(th,dbeta(th,0.5,0.5),ylim=c(0,30),col="darkgrey",t="l")   #Beta(0.5,0.5)
plot(th,dbeta(th,1000,1000*(1-0.4854369)/0.4854369),ylim=c(0,400),col="darkgrey",t="l") #informative prior with expectation 0.4854369
```

  f. How does our inference change when we take sample size $n=50$, $n=100$, $n=1000$?
  e. Use prior $a=0.5, b=0.5$ 
  
# Specifying a binomial model using `Stan` and `RStan`.
There is different software to carry out Bayesian inference. One of the most recent and popular software is [`Stan`](https://mc-stan.org/), which we are using in this lab for estimating the probability that aa newborn in a given hospital is a girl as in the previous section. 

## Basics of `Stan` and `RStan`
To use `Stan` in `R`, we need the to install and load the `RStan` package. Depending on your computer, you will be asked to install `Rtools`. Install it and be patient, given that it takes a little bit of time. Do not forget to set up your working directory as well.
```{r warning = F, message = F, eval = F}
# install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies=TRUE)
library("rstan")
```

```{r}
library("rstan")
```

You can write a `Stan` program in a text editor, saving them with the extension `.stan` at the end of the file name (e.g. `model.stan`). You can also write a `Stan` program in RStudio as we are going to do in this lab.

## Writing a model in `Stan`
Stan works in six blocks of code, which correspond to `data`, `parameters`, `model`, `transformed data`, `transformed parameters`, and `generated quantities`. You need to define at least the three first blocks.

- The `data` block comprehends the input data. You need to define all the elements that your model requires. For our case, the elements are the sample size and the data denoting whether a girl or boy is born.

- The `parameters` block includes the parameters of our interest. In out case is $\theta$, which is bounded between $0$ and $1$ due to being a probability. 

- In the `model` block, you need to specify not only the likelihood for a model but also its priors.

```{r eval=T, echo=T}
stan_model = "
data {
  int<lower=0> N; //sample size with a minimum value of 0
  int k;          //data denoting whether a girl or boy is born
  }
transformed data {
  }
parameters {
  real<lower=0, upper=1>  theta;    //probability of having a girl
  }
transformed parameters {
  }
model {
  // Priors
  theta ~ beta(1,1);    //the same as uniform!
  
  // Likelihood
  k ~ binomial(N, theta);
  }
generated quantities{
}"
writeLines(stan_model, con = "stan_model.stan" )
```

There are three blocks which are empty in the `Stan` model above:

- The `transformed data` block, in which you can specify any transformation of the input data.

- The `transformed parameters` block which enables the definition of intermediate parameters when the likelihood requires to be specified in a different ways. 

- The `generated quantities` block, from which you can make predictions based on the resulting posterior.

## Formatting input data and initial values for `Stan` model
Once the model is specified, you need to format the input data to be used. `Stan` accepts data in a `list` format as follows  

```{r echo=T, eval=T}
data.inp <- list(k = sum(girl[1:20]), 
                 N = 20)         
```

To obtain easily convergence of the model, you can also define initial values. In our case, it is sensible to include $0.5$ as an initial value of our parameter $\theta$.

```{r echo=T, eval=T}
inits00 <- list(theta=0.5)
```

## Model estimation
```{r echo=T, eval=T, warning = F, message = F}
fit.bin = stan(file = "stan_model.stan", 
                data = data.inp, 
                iter = 1000, 
                thin = 1, 
                warmup = 500,
                verbose = FALSE, 
                init = list(inits00, inits00),   # a list of two as two chains
                chains = 2, cores = 2, 
                seed = 26)
```

## Basic diagnostics: convergence
Before looking at the posterior distribution, you need to perform basic diagnostics. The main aspect that you need to look at is the convergence of the parameters of your interest. This can be done visually, using the following code:

```{r echo=T, eval=T, warning = F, message = F}
plot(fit.bin, plotfun = "trace", pars = c("theta"), inc_warmup = F) 
```

Convergence can be also assessed numerically, using the $\hat{R}$ values, which should be smaller than $1.1$. To obtainthe $\hat{R}$ values, you need to install and load the package `coda`. With this package, you can plot these values, but you need to convert the stan object to a coda object firstly as the following code shows:

```{r echo=T, eval=T, warning = F, message = F}
#install.packages("coda")
library("coda")

coda_object = As.mcmc.list(fit.bin)
gelman.plot(coda_object[,c("theta")])
```

## Visualising the posterior distribution
Once we have made sure that our chains have mixed well, we can look at the posterior distribution of our parameter $\theta$ using a density plot.

```{r echo=T, eval=T, warning = F, message = F}
plot(fit.bin, plotfun = "dens", pars = c("theta"), inc_warmup = F) + scale_x_continuous(limits = c(0, 1))
```

You can look at other statistics of out posterior distribution using the `summary` function.

```{r echo=T, eval=T, warning = F, message = F}
print(fit.bin)
```
## Your turn
Re-run the binomial model in `Stan` with a prior different from the $Beta$ distribution. What are the differences of the posterior from this model and the one used previously?

# Exercise
Now, we are interested in looking at the average age of the mothers of the first thousand of babies born in some hospital since the beginning of year 2017. We define that the average age of mothers $y \sim N(\mu, \sigma^2)$.

a. Simulate the thousand values (i.e. the input data for the model), assuming that the mean age of mothers is $27$ and $sd=2$.

b. Make inferences for the mean and variance of the mothers' ages using a vague prior and an informative prior. 

<!-- *This example will be demonstrated in the Lab. However, it will be great if you could attempt coding on your own before the Lab and testing if `RStan` works on your machine.* -->
